---
title: "Dn Metric Practice"
output: word_document
editor_options: 
  chunk_output_type: console
---


## Langenbrunner's Dn Metric 

The Draft 2006 technical paper uses a slightly different one then than what the Blanco and Hunkle Sea Ice Model. 

In the Draft paper the normalized difference is weighted by $\frac{1}{n-param}$ whereas in Blanco and Hunkle Sea Ice the normalized difference is weighted by $\frac{1}{n}$ . I think that this is pretty trivial and that we can go with the Blanco and Hunkle Sea Ice method. 


The big idea is that ${D}_{n}$ is a $\gamma$ distributed metric that takes into account both model and observational variablitly when judging the error between the model and observational data. 


##### Part 1
$${D}_{n} = \frac{1}{n}\sum_{i = 1}^{n}\frac{({O}_{i} - {M}_{i}) ^ 2}{{s}_{i}^2}$$

** Equation 1:** This calculates ${D}_{n}$ the distance metric between an observation data point and a model data point nomalized by ${s}_{i}^2$ some sort of variability index. In the Sea Ice Model paper this was set equal to the sd of the the observational data we will probably want to do the same. 


#### Part 2

One of the advantages to using this method is that ${D}_{n}$ has a $\gamma$ distribution so we can judge the model output "matches" observations on some significance level. 


We can figure out what the distribtuion of ${D}_{n}$ or $\gamma({D}_{n}, {a}, {b})$ with $a = \frac{n}{2}$ and $b = \frac{n}{2}^2 \sum_{i = 1}^{n}\frac{\sigma^2_{(O-M)}}{s_{i}^2} $


##### Things we will need to decide on 

- ${s}_{i}$ : in the sea ice paper they chose this value to represent the observational variablity and set it equal to the sd
- $\sigma_{{O-M}}$ : in the sea ice paper was set to account for spatial variability and they mention that it could be set up to allow for temporal variability. Otherwise we could assume that there is no model variability but this would make the "matching" criteria harder. 
- $\alpha$ : the significance level for the testing



## Atm CO2 

```{r set up, echo=F}
library(dplyr)
library(tidyr)
library(readr)
library(purrr)
library(ggplot2)

BASE <- "C:/Users/dorh012/Documents/hector-SA-npar"
```



```{r import CO2 data, message=F, warning=F}

# Import and format data.
obs_path    <- file.path(BASE, "input", "observations", "NOAA_co2_mm_mlo.txt")
data        <- read.table(obs_path) 
names(data) <- c("year", "month" , "decimal_date", "average", "interpolated", "trend", "days")

# Subset the data so that it only includes the complete years.
complete_years <- 1959:2017
data           <- filter(data, year %in% complete_years)

# Calculate the annual average and the variability within each year. 
# TODO: do we want to use a constant sd for si? average variability or max variability?
data %>% 
  group_by(year) %>% 
  summarise(value = mean(interpolated), sd = sd(interpolated)) %>%  
  mutate(sd =  sd) %>% 
  ungroup %>%
  mutate(variable = "atm_CO2", 
         units = "ppm") -> 
  annual_data
  
```



```{r import hector data, message=F, warning=F}
# read in hector CO2 
hector_data <- readr::read_csv( file.path(BASE, 'int-out', 'rcp26', 'C.Ca_hector_run_cleanup.csv') ) %>% filter(year %in% annual_data$year)
```


```{r}
# 
# # Okay I did not know what is going on with my funcitons... probably not properly stcutured... this method may be even more 
# # restrictive... may be for error bars I should look at doing a puls and minus 5 percent of the values... idk what is going 
# # on worried about the Sn and also the Dc slectiong 
# 
# 
# # Dn_func <- function(n, obs, model, s2n){
# #   (1 / (n) ) * sum( ((obs - model)^2) / s2n)
# # }
# 
# 
# Dn_func <- function(data){
#   
#   if(any(!c("obs", "mod", "s2n") %in% names(data))){stop("Missing a required column.")}
#   
#   run_name <- unique(data$run_name)
#   n <- nrow(data)
#   
#   diff_sq    <- (data$obs - data$mod)^2
#   normalized <- diff_sq / data$s2n
#   
#   Dn <- (1 / n) * sum(normalized)
#   
# 
#   tibble(run_name = run_name, Dn = Dn)
#   
# }
# 
# 
# 
# b_func <- function(n, sigma, sn){
#   (1/n^2) * sum(sigma / sn)
# }
# 
# sigma_func <- function(obs, model){
#   
#   var(obs) + var(model) - 2 * cov(obs, model)
#   
#   }
#   
#   
#   
# Dc_function <- function(Dn, obs, mod, alpha = 0.05){
#   
#   n      <- length(obs)
#   a     <- 1/n 
#   sigma <- var(obs) + var(model) - 2 * cov(obs, model)
#   b     <-  (1/n^2) * sum(sigma / sn)
#   
#   to <- unique(mod$Dn) * 4
#   x <- seq(0, to , length.out = 1e3)
#   CDF <- pgamma(x, shape = a, scale = b)
#   
#   difference <- abs(CDF - (1 - alpha))
#   index <- which.min(difference)
#   
#   Dc <- x[index]
#   tibble(run_name = unique(mod[['run_name']]), Dc = Dc)
# 
# 
# }
# 
#  hector_data %>% 
#    rename(mod = value) %>% 
#    full_join( select(annual_data, year, obs = value, s2n = sd) , by = "year") -> 
#    mod_obs_sd
# 
# run_list <- split(mod_obs_sd, mod_obs_sd$run_name)
# 
# map(run_list[1:5], Dn_func) %>% 
#   bind_rows -> 
#   Dn_values
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# Dn_list <- split(Dn_hector, Dn_hector$run_name)
# map(Dn_list, function(Dn_list){
#   Dc_function(Dn_list[['Dn']], obs = annual_data, mod = Dn_list)
# }) %>%  
#   bind_rows %>% 
#   full_join(Dn_hector) -> 
#   test
# 
#  test$Dn %>% min
#  test$Dc %>% max
# 


```

```{r}
hector_data %>% 
            rename(cal = value) %>% 
  full_join(annual_data %>% 
              select(year, obs = value, s2n = sd), by = "year") -> complete_data

complete_data %>% 
  split(.$run_name) %>% 
  map(function(data = .){
    
    run_name <- unique(data$run_name)
    n        <- nrow(data)
    Dn       <- (1 / n) * sum( ((data[['obs']] - data[['cal']])^2) / data[['s2n']])
    
    tibble(run_name = run_name, Dn = Dn )
    
  }) %>% 
  bind_rows -> 
  Dn_data

  Dn_data %>% 
    full_join(complete_data, by = "run_name") -> 
    Dn_complete_data

  Dn_complete_data %>% 
    split(.$run_name) -> list
  
  list %>% 
    map(function(data = .){
      
      run_name <- unique(data[['run_name']])
      
        n <- nrow(data)
        a <- n / 2
        
       # sigma_diff <- var(data[['obs']]) + var(data[['cal']]) + 2 * cov(data[['obs']], data[['cal']])
        # Assume that Cs are fixed... idk why you would do that 
        sigma_diff <- sd(data[['obs']]) 
        si2        <- data[['s2n']]
        b          <- (1/n^2) * sum(sigma_diff / si2)
    
        Dn <- unique(data[['Dn']])
        
        x   <- seq(0, Dn * 3, length.out = 1000)
        Dc <- qgamma((1 - 0.05), shape = a, scale = b)
        
        tibble(run_name = run_name, Dn = Dn, Dc = Dc )
        
    }) %>%  
    bind_rows -> 
    x

  
  # Something must be wrong because when I decrease alpha we are seeing more and more runs pass 
  # with significant matching... in theorly this should be the opposite 

```


```{r}

sig_runs <- filter(x, Dn <= Dc)

ggplot() + 
  geom_line(data = hector_data, aes(year, value), color = "black", alpha = 0.2) + 
  geom_line(data = annual_data, aes(year, value), color = "blue") + 
  geom_ribbon(data = annual_data, aes(x = year, ymin = value - sd, ymax = value + sd), alpha = 0.3) + 
  geom_line(data = hector_data %>% filter(run_name %in% c("hectorSA-1560", "hectorSA-4671")), 
            aes(year, value, group = run_name), color = "red")  +
  geom_line(data = hector_data %>% filter(run_name %in% sig_runs$run_name), 
            aes(year, value, group = run_name), color = "purple")


```




